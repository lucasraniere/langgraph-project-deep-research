{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85b6d580",
   "metadata": {},
   "source": [
    "### **User Clarification and Brief Generation**\n",
    "\n",
    "*The goal of scoping is to gather user-context needed for research.*\n",
    "\n",
    "The research is scopped in two phases:\n",
    "- **User Clarifiction** - Determines if additional clarification is needed from the user\n",
    "- **Brief Generation** - Transform the conversation into a detailed research brief\n",
    "\n",
    "#### **State and Schemas**\n",
    "\n",
    "First, we'll define the state object and schemas for our process.\n",
    "\n",
    "The state object serves as our primary mechanism for storing and passing context between different phrases of the research workflow.\n",
    "\n",
    "We can use it to write and select context that will be used to guide the research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "397681cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/states/state_scope.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/states/state_scope.py\n",
    "\n",
    "\"\"\"\n",
    "State Definitions and Pydantic Schemas for Research Scoping\n",
    "\n",
    "This defines the state objects and structured schemas used for\n",
    "the research agent scoping workflow, includin researcher state management and output schemas.\n",
    "\"\"\"\n",
    "\n",
    "import operator\n",
    "from typing_extensions import Optional, Annotated, List, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# State Definitions\n",
    "\n",
    "class AgentInputState(MessagesState):\n",
    "    \"\"\"Input state for the full agent - only contains messages from user input.\"\"\"\n",
    "    pass\n",
    "\n",
    "class AgentState(MessagesState):\n",
    "    \"\"\"\n",
    "    Main state for the full multi-agent research system.\n",
    "\n",
    "    Extends MessagesStates with additional fields for research coordination.\n",
    "    Note: Some fields are duplicated across different state classes for proper\n",
    "    state managment between subgraphs and the main workflow.\n",
    "    \"\"\"\n",
    "\n",
    "    # Research brief generated from user conversation history\n",
    "    research_brief: Optional[str]\n",
    "    # Messages exchanged with the supervisor agent for coordination\n",
    "    supervisor_messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    # Raw unprocessed research notes collected during the research phase\n",
    "    raw_notes: Annotated[list[str], operator.add] = []\n",
    "    # Processed and strctured notes ready for report generation\n",
    "    notes: Annotated[list[str], operator.add] = []\n",
    "    # Final formatted research report\n",
    "    final_report: str\n",
    "\n",
    "# Structured Output Schemas\n",
    "\n",
    "class ClarifyWithUser(BaseModel):\n",
    "    need_clarification: bool = Field(\n",
    "        description=\"Whether the user needs to be asked a clarifying question.\",\n",
    "    )\n",
    "    question: str = Field(\n",
    "        description=\"A question to ask the user to clarify the report scope.\",\n",
    "    )\n",
    "    verification: str = Field(\n",
    "        description=\"Verify message that we will start research after the user has provided the necessary information.\",\n",
    "    )\n",
    "\n",
    "class ResearchQuestion(BaseModel):\n",
    "    research_brief: str = Field(\n",
    "        description=\"A research question that will be used to guide the research.\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6068c7f3",
   "metadata": {},
   "source": [
    "#### **Scope Research**\n",
    "\n",
    "Now, we'll create a simple workflow to clarify user's intent and write a research brief.\n",
    "\n",
    "We'll let the LLM determine whether it has sufficient clarification to write the brief.\n",
    "\n",
    "This will use LangGraph's Command to direct the control flow and updating state. The `Command` object takes two key parameters:\n",
    "- `goto`: Specifies the next node to execute (or `END` to terminate)\n",
    "- `update`: Dictionary of states updates to apply before transitioning\n",
    "\n",
    "This pattern allows our functions to both process data and direct the workflow based on their results.\n",
    "\n",
    "It creates a more flexible and maintainable system than traditional static graph structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1476f5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../src/research_agent_scope.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../src/research_agent_scope.py\n",
    "\n",
    "\"\"\"\n",
    "User clarification and Research Brief Generation\n",
    "\n",
    "This module implements the scoping phase of the research workflow, where we:\n",
    "1. Assess if the user's requests needs clarification\n",
    "2. Generate a detailed research brief from the conversation\n",
    "\n",
    "The workflow uses structured output to make deterministic decisions abou\n",
    "whether sufficient context existis to proceed with research.\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "from typing_extensions import Literal\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, AIMessage, get_buffer_string\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "\n",
    "from prompts import clarify_with_user_instructions, transform_messages_into_research_topic_prompt\n",
    "from states.state_scope import AgentState, ClarifyWithUser, ResearchQuestion, AgentInputState\n",
    "\n",
    "from langchain_core.rate_limiters import InMemoryRateLimiter\n",
    "\n",
    "import os\n",
    "os.chdir(os.path.dirname(os.path.realpath(__file__)))\n",
    "\n",
    "# == Utility Functions ==\n",
    "def get_today_str() -> str:\n",
    "    \"\"\"Get current date in a human-readable format.\"\"\"\n",
    "    return datetime.now().strftime(\"%a %b %-d, %Y\")\n",
    "\n",
    "\n",
    "# == Configuration ==\n",
    "\n",
    "# Initialize model\n",
    "rate_limiter = InMemoryRateLimiter(\n",
    "    requests_per_second=8.3,\n",
    "    check_every_n_seconds=0.1,\n",
    "    #max_bucket_size=10\n",
    ")\n",
    "model = init_chat_model(model=\"openai:gpt-4.1\", temperature=0.0, rate_limiter=rate_limiter)\n",
    "\n",
    "# Nodes\n",
    "def clarify_with_user(state: AgentState) -> Command[Literal[\"write_research_brief\", \"__end__\"]]:\n",
    "    \"\"\"\n",
    "    Clarification decision node.\n",
    "\n",
    "    Determines if the user's request contains sufficient information to proceed\n",
    "    with research or if additional clarification is need.\n",
    "\n",
    "    Uses structured output to make deterministic decisions and avoid hallucination.\n",
    "    Routes to either research brief generation of ends with a clarification question.\n",
    "    \"\"\"\n",
    "    # Set up structured output model\n",
    "    structured_output_model = model.with_structured_output(ClarifyWithUser)\n",
    "\n",
    "    # Invoke the model with clarification instructions\n",
    "    response = structured_output_model.invoke([\n",
    "        HumanMessage(content=clarify_with_user_instructions.format(\n",
    "            messages=get_buffer_string(messages=state[\"messages\"]),\n",
    "            date=get_today_str()\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Route based on clarification need\n",
    "    if response.need_clarification:\n",
    "        return Command(\n",
    "            goto=END,\n",
    "            update={\"messages\": [AIMessage(content=response.question)]}\n",
    "        )\n",
    "    else:\n",
    "        return Command(\n",
    "            goto=\"write_research_brief\",\n",
    "            update={\"messages\": [AIMessage(content=response.verification)]}\n",
    "        )\n",
    "\n",
    "\n",
    "def write_research_brief(state: AgentState):\n",
    "    \"\"\"\n",
    "    Research brief generation node.\n",
    "\n",
    "    Transforms the conversation history into a detailed research brief\n",
    "    that will guide the subsequent research phase.\n",
    "\n",
    "    Uses structured output to ensure the brief follow the required format\n",
    "    and contains all necessary details for effective research.\n",
    "    \"\"\"\n",
    "    # Set up structured output model\n",
    "    structured_output_model = model.with_structured_output(ResearchQuestion)\n",
    "\n",
    "    # Generate research brief from conversation history\n",
    "    response = structured_output_model.invoke([\n",
    "        HumanMessage(content=transform_messages_into_research_topic_prompt.format(\n",
    "            messages=get_buffer_string(messages=state[\"messages\"]),\n",
    "            date=get_today_str()\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Update state with generated research brief and pass it to the supervisor\n",
    "    return {\n",
    "        \"research_brief\": response.research_brief,\n",
    "        \"supervisor_messages\": [HumanMessage(content=f\"{response.research_brief}.\")]\n",
    "    }\n",
    "\n",
    "# == Graph Construcion ==\n",
    "\n",
    "# Building the scoping workflow\n",
    "deep_researcher_builder = StateGraph(AgentState, input_schema=AgentInputState)\n",
    "\n",
    "# Add nodes\n",
    "deep_researcher_builder.add_node(\"clarify_with_user\", clarify_with_user)\n",
    "deep_researcher_builder.add_node(\"write_research_brief\", write_research_brief)\n",
    "\n",
    "# Add edges\n",
    "deep_researcher_builder.add_edge(START, \"clarify_with_user\")\n",
    "deep_researcher_builder.add_edge(\"write_research_brief\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "scope_research = deep_researcher_builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4db1613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compile with in-memory checkpointer to test in notebook\n",
    "from IPython.display import Image, display\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "from research_agent_scope import deep_researcher_builder\n",
    "\n",
    "checkpointer = InMemorySaver()\n",
    "scope = deep_researcher_builder.compile(checkpointer=checkpointer)\n",
    "display(Image(scope.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62c50558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭─────────────────────────────────────────────────── 🧑 Human ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> I want to research the best coffe shops in San Francisco.                                                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m──────────────────────────────────────────────────\u001b[0m\u001b[34m 🧑 Human \u001b[0m\u001b[34m───────────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m I want to research the best coffe shops in San Francisco.                                                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Could you please specify what criteria are most important to you when determining the 'best' coffee shops? For  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> example, are you interested in ambiance, specialty coffee, food options, Wi-Fi availability, or something else? <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m Could you please specify what criteria are most important to you when determining the 'best' coffee shops? For  \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m example, are you interested in ambiance, specialty coffee, food options, Wi-Fi availability, or something else? \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run the full workflow\n",
    "from utils import format_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "result = scope.invoke({\"messages\": [HumanMessage(\n",
    "    content=\"I want to research the best coffe shops in San Francisco.\"\n",
    ")]}, config=thread)\n",
    "\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74935553",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭─────────────────────────────────────────────────── 🧑 Human ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> I want to research the best coffe shops in San Francisco.                                                       <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m──────────────────────────────────────────────────\u001b[0m\u001b[34m 🧑 Human \u001b[0m\u001b[34m───────────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m I want to research the best coffe shops in San Francisco.                                                       \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Could you please specify what criteria are most important to you when determining the 'best' coffee shops? For  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> example, are you interested in ambiance, specialty coffee, food options, Wi-Fi availability, or something else? <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m Could you please specify what criteria are most important to you when determining the 'best' coffee shops? For  \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m example, are you interested in ambiance, specialty coffee, food options, Wi-Fi availability, or something else? \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #000080; text-decoration-color: #000080\">╭─────────────────────────────────────────────────── 🧑 Human ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">│</span> Let's examine coffee quality to asses the best coffee shops in San Francisco.                                   <span style=\"color: #000080; text-decoration-color: #000080\">│</span>\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[34m╭─\u001b[0m\u001b[34m──────────────────────────────────────────────────\u001b[0m\u001b[34m 🧑 Human \u001b[0m\u001b[34m───────────────────────────────────────────────────\u001b[0m\u001b[34m─╮\u001b[0m\n",
       "\u001b[34m│\u001b[0m Let's examine coffee quality to asses the best coffee shops in San Francisco.                                   \u001b[34m│\u001b[0m\n",
       "\u001b[34m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╭───────────────────────────────────────────────────── 📝 AI ─────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Thank you for clarifying that you would like to focus on coffee quality when assessing the best coffee shops in <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> San Francisco. I now have sufficient information to proceed and will begin researching top coffee shops in San  <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span> Francisco based on their coffee quality.                                                                        <span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">│</span>\n",
       "<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[37m╭─\u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m 📝 AI \u001b[0m\u001b[37m────────────────────────────────────────────────────\u001b[0m\u001b[37m─╮\u001b[0m\n",
       "\u001b[37m│\u001b[0m Thank you for clarifying that you would like to focus on coffee quality when assessing the best coffee shops in \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m San Francisco. I now have sufficient information to proceed and will begin researching top coffee shops in San  \u001b[37m│\u001b[0m\n",
       "\u001b[37m│\u001b[0m Francisco based on their coffee quality.                                                                        \u001b[37m│\u001b[0m\n",
       "\u001b[37m╰─────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = scope.invoke({\"messages\": [HumanMessage(\n",
    "    content=\"Let's examine coffee quality to asses the best coffee shops in San Francisco.\"\n",
    ")]}, config=thread)\n",
    "\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf58412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I want to identify the best coffee shops in San Francisco, focusing specifically on coffee quality as the primary  \n",
       "criterion. In my research, I would like to consider factors that contribute to coffee quality, such as bean        \n",
       "sourcing, roasting methods, brewing techniques, and taste. Other aspects like ambiance, food options, Wi-Fi        \n",
       "availability, or price are not specified as important for this research and can be treated as flexible or open     \n",
       "considerations. Please prioritize information from official coffee shop websites, reputable specialty coffee       \n",
       "publications, and user reviews from primary platforms (such as Google Reviews or Yelp) that specifically address   \n",
       "coffee quality.                                                                                                    \n",
       "</pre>\n"
      ],
      "text/plain": [
       "I want to identify the best coffee shops in San Francisco, focusing specifically on coffee quality as the primary  \n",
       "criterion. In my research, I would like to consider factors that contribute to coffee quality, such as bean        \n",
       "sourcing, roasting methods, brewing techniques, and taste. Other aspects like ambiance, food options, Wi-Fi        \n",
       "availability, or price are not specified as important for this research and can be treated as flexible or open     \n",
       "considerations. Please prioritize information from official coffee shop websites, reputable specialty coffee       \n",
       "publications, and user reviews from primary platforms (such as Google Reviews or Yelp) that specifically address   \n",
       "coffee quality.                                                                                                    \n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rich.markdown import Markdown\n",
    "Markdown(result['research_brief'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff899f6f",
   "metadata": {},
   "source": [
    "### **Evaluation**\n",
    "\n",
    "Now that we've scoped our research, let's test it with a few examples to make sure that it's working as expected.\n",
    "\n",
    "Let's think about what makes for a good research brief:\n",
    "\n",
    "- It captures relevant criteria from the user chat\n",
    "- It does not invent of assume any criteria that the user did not explicitly provide\n",
    "\n",
    "Here are two sample input conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66ddc6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "conversation_1 = [\n",
    "    HumanMessage(content=\"What's the best way to invest $50,000 for retirement?\"),\n",
    "    AIMessage(content=\"Could you please provide some additional information to tailor the investment advice for your $50,000 retirement goal? Specifically:\\n Your current age or desired retirement age\\n Your risk tolerance (low, medium, high)\\n Any preferences for investment types (e.g., stocks, bonds, mutual funds, real estate)\\n Whether you are investing through a tax-advantaged account (e.g., IRA, 401(k)) or a regular brokerage account\\n This will help me provide more personalized and relevant suggestions.\"),\n",
    "    HumanMessage(content=\"I'm 25 and I want to retire by 45. My risk tolerance is high right now but I think will decrease over time. I have heard that stocks and ETFs are a good choice, but I'm open to anything. And I already have a 401k, but this would just be through a regular brokerage account.\"),\n",
    "]\n",
    "\n",
    "conversation_2 = [\n",
    "    HumanMessage(content=\"I am looking for an apartment in NYC, can you help me?\"),\n",
    "    AIMessage(content=\"Could you please specify your apartment preferences? For example:\\n Desired neighborhoods or boroughs\\n Number of bedrooms/bathrooms\\n Budget range (monthly rent)\\n Any amenities or must-have features\\n Preferred move-in date\\n This information will help me provide the most relevant apartment options in NYC.\"),\n",
    "    HumanMessage(content=\"I'd prefer to live in Chelsea, Flatiron, or West Village. I'm looking for a 2 bed 2 bath, and I am looking for monthly rent below 7k. I'd like this to be a doorman building and have an in unit washer and dryer, but it's okay if there's no washer dryer. It's a plus if the building has a gym. And I'd like to move in in September 2025.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f7f02",
   "metadata": {},
   "source": [
    "Now let's manually write out each criteria from these conversations that we would want preserved by a research brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a92ee082",
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria_1 = [\n",
    "    \"Current age is 25\",\n",
    "    \"Desired retirement age is 45\",\n",
    "    \"Current risk tolerance is high\",\n",
    "    \"Interested in investing in stocks and ETFs\",\n",
    "    \"Open to forms of investment beyond stocks and ETFs\"\n",
    "    \"Investment account is a regular brokerage account\",\n",
    "]\n",
    "\n",
    "criteria_2 = [\n",
    "    \"Looking for a 2 bed 2 bath apartment in Chelsea, Flatiron, or West Village\",\n",
    "    \"Monthly rent below 7k\",\n",
    "    \"Should be in a doorman building\",\n",
    "    \"Ideally have an in unit washer and dryer but not strict\",\n",
    "    \"Ideally have a gym but not strict\",\n",
    "    \"Move in date is September 2025\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c89f3",
   "metadata": {},
   "source": [
    "We're going to use LangSmith to run this experiment.\n",
    "\n",
    "Running an experiment in LangSmith comprises of three steps:\n",
    "1. Creating the dataset\n",
    "2. Writing the evaluator(s)\n",
    "3. Running the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad89d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import Client\n",
    "\n",
    "# Initialize the LangSmith client\n",
    "langsmith_client = Client(api_key=os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "\n",
    "# Create the dataset\n",
    "dataset_name = \"deep_research_scoping\"\n",
    "if not langsmith_client.has_dataset(dataset_name=dataset_name):\n",
    "\n",
    "    # Create the dataset\n",
    "    dataset = langsmith_client.create_dataset(\n",
    "        dataset_name=dataset_name,\n",
    "        description=\"A dataset that measures the quality of research briefs generated from an input conversation\",\n",
    "    )\n",
    "\n",
    "    # Add the examples to the dataset\n",
    "    langsmith_client.create_examples(\n",
    "        dataset_id=dataset.id,\n",
    "        examples=[\n",
    "            {\n",
    "                \"inputs\": {\"messages\": conversation_1},\n",
    "                \"outputs\": {\"criteria\": criteria_1},\n",
    "            },\n",
    "            {\n",
    "                \"inputs\": {\"messages\": conversation_2},\n",
    "                \"outputs\": {\"criteria\": criteria_2},\n",
    "            },\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdba5b",
   "metadata": {},
   "source": [
    "Now, we need to write an evaluator that will compare our research brief against the success criteria that we have specified for each example. For this, we'll use an LLM-as-judge. You can fine some useful tips for writing llm-as-judge evaluators here, which include:\n",
    "\n",
    "1. **Role Definition with Expertise Context**\n",
    "\n",
    "- Defined specific expert roles (\"research brief evaluator\", \"meticulous auditor\")\n",
    "- Specialized the role to the specific evaluation domain\n",
    "\n",
    "2. **Clear Task Specification**\n",
    "\n",
    "- Binary pass/fail judgments (avoiding complex multi-dimensional scoring)\n",
    "- Explicit task boundaries and objectives\n",
    "- Focus on actionable evaluation criteria\n",
    "\n",
    "3. **Rich Contextual Background**\n",
    "\n",
    "- Provide domain-specific context about research brief quality\n",
    "- Explain the importance of accurate evaluation\n",
    "- Connect evaluation outcomes to downstream consequences\n",
    "\n",
    "4. **Structured XML Organization**\n",
    "\n",
    "- Used semantic XML tags for different sections\n",
    "- Clear separation of role, task, context, inputs, guidelines, and outputs\n",
    "- Improved prompt parsing and comprehension\n",
    "\n",
    "5. **Comprehensive Guidelines with Examples**\n",
    "\n",
    "- Detailed PASS/FAIL criteria with specific conditions\n",
    "- Multiple concrete examples showing correct judgments\n",
    "- 3-4 examples per prompt covering different scenarios\n",
    "- Both positive and negative examples for each judgment type\n",
    "- Edge case handling and decision boundary clarification\n",
    "\n",
    "6. **Explicit Output Instructions**\n",
    "\n",
    "- Clear guidance on how to apply the evaluation criteria\n",
    "- Instructions for handling ambiguous cases\n",
    "- Emphasis on consistency and systematic evaluation\n",
    "\n",
    "7. **Bias Reduction Techniques**\n",
    "\n",
    "- \"Strict but fair\" guidance to balance precision and recall\n",
    "- \"When in doubt, lean toward FAIL\" for conservative evaluation\n",
    "- Systematic evaluation process to reduce subjective variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1da6db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import BRIEF_CRITERIA_PROMPT\n",
    "from typing_extensions import cast\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Criteria(BaseModel):\n",
    "    \"\"\"\n",
    "    Individual success criteria evaluation result.\n",
    "\n",
    "    This model represents a single evaluation criteria that should be present\n",
    "    in the research brief, along with a detailed assessment of whether it was\n",
    "    successfully captured and the reasoning behind that assessment.\n",
    "    \"\"\"\n",
    "    criteria_text: str = Field(\n",
    "        description=\"The specific success criteria being evaluated (e.g., 'Current age is 25', 'Monthly rent below 7k')\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Detailed explanation of why this criteria is or isn't captured in the research brief, including specific evidence from the brief\"\n",
    "    )\n",
    "    is_captured: bool = Field(\n",
    "        description=\"Whether this specific criteria is adequately captured in the research brief (True) or missing/inadequately addressed (False)\"\n",
    "    )\n",
    "\n",
    "def evaluate_success_criteria(outputs: dict, reference_outputs: dict):\n",
    "    \"\"\"\n",
    "    Evaluate whether the research brief captures all required success criteria.\n",
    "\n",
    "    This function evaluates each criterion individually to provide focused assessment\n",
    "    and detailed reasoning for each evaluation decision.\n",
    "\n",
    "    Args:\n",
    "        outputs: Dictionary containing the research brief to evaluate\n",
    "        reference_outputs: Dictionary containing the list of success criteria\n",
    "\n",
    "    Returns:\n",
    "        Dict with evaluation results including score (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    research_brief = outputs[\"research_brief\"]\n",
    "    success_criteria = reference_outputs[\"criteria\"]\n",
    "\n",
    "    model = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "    structured_output_model = model.with_structured_output(Criteria)\n",
    "\n",
    "    # Run evals\n",
    "    responses = structured_output_model.batch([\n",
    "    [\n",
    "        HumanMessage(\n",
    "            content=BRIEF_CRITERIA_PROMPT.format(\n",
    "                research_brief=research_brief,\n",
    "                criterion=criterion\n",
    "            )\n",
    "        )\n",
    "    ] \n",
    "    for criterion in success_criteria])\n",
    "    \n",
    "    # Ensure the criteria_text field is populated correctly\n",
    "    individual_evaluations = [\n",
    "        Criteria(\n",
    "            reasoning=response.reasoning,\n",
    "            criteria_text=criterion,\n",
    "            is_captured=response.is_captured\n",
    "        )\n",
    "        for criterion, response in zip(success_criteria, responses)\n",
    "    ]\n",
    "    \n",
    "    # Calculate overall score as percentage of captured criteria\n",
    "    captured_count = sum(1 for eval_result in individual_evaluations if eval_result.is_captured)\n",
    "    total_count = len(individual_evaluations)\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"success_criteria_score\", \n",
    "        \"score\": captured_count / total_count if total_count > 0 else 0.0,\n",
    "        \"individual_evaluations\": [\n",
    "            {\n",
    "                \"criteria\": eval_result.criteria_text,\n",
    "                \"captured\": eval_result.is_captured,\n",
    "                \"reasoning\": eval_result.reasoning\n",
    "            }\n",
    "            for eval_result in individual_evaluations\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad57629",
   "metadata": {},
   "source": [
    "Our second evaluator will check that the research brief does not make any assumptions that the user did not specify in the research brief."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaadeb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import BRIEF_HALLUCINATION_PROMPT\n",
    "\n",
    "# Improved NoAssumptions class with reasoning field and enhanced descriptions\n",
    "class NoAssumptions(BaseModel):\n",
    "    \"\"\"\n",
    "    Evaluation model for checking if research brief makes unwarranted assumptions.\n",
    "\n",
    "    This model evaluates whether the research brief contains any assumptions,\n",
    "    inferences, or additions that were not explicitly stated by the user in their\n",
    "    original conversation. It provides detailed reasoning for the evaluation decision.\n",
    "    \"\"\"\n",
    "    no_assumptions: bool = Field(\n",
    "        description=\"Whether the research brief avoids making unwarranted assumptions. True if the brief only includes information explicitly provided by the user, False if it makes assumptions beyond what was stated.\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"Detailed explanation of the evaluation decision, including specific examples of any assumptions found or confirmation that no assumptions were made beyond the user's explicit statements.\"\n",
    "    )\n",
    "\n",
    "def evaluate_no_assumptions(outputs: dict, reference_outputs: dict):\n",
    "    \"\"\"\n",
    "    Evaluate whether the research brief avoids making unwarranted assumptions.\n",
    "\n",
    "    This evaluator checks that the research brief only includes information\n",
    "    and requirements that were explicitly provided by the user, without\n",
    "    making assumptions about unstated preferences or requirements.\n",
    "\n",
    "    Args:\n",
    "        outputs: Dictionary containing the research brief to evaluate\n",
    "        reference_outputs: Dictionary containing the success criteria for reference\n",
    "\n",
    "    Returns:\n",
    "        Dict with evaluation results including boolean score and detailed reasoning\n",
    "    \"\"\"\n",
    "    research_brief = outputs[\"research_brief\"]\n",
    "    success_criteria = reference_outputs[\"criteria\"]\n",
    "\n",
    "    model = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "    structured_output_model = model.with_structured_output(NoAssumptions)\n",
    "\n",
    "    response = structured_output_model.invoke([\n",
    "        HumanMessage(content=BRIEF_HALLUCINATION_PROMPT.format(\n",
    "            research_brief=research_brief,\n",
    "            success_criteria=str(success_criteria)\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"key\": \"no_assumptions_score\",\n",
    "        \"score\": response.no_assumptions,\n",
    "        \"reasoning\": response.reasoning\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee97e0a",
   "metadata": {},
   "source": [
    "Now that we have our evaluators, we can run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a201ce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'Deep Research Scoping-edadbddf' at:\n",
      "https://smith.langchain.com/o/29030f9f-1b1f-45f1-b0d1-be78131813d8/datasets/30bc97b3-5468-4444-aeff-a414f0a05102/compare?selectedSessions=f2e4474e-4d99-4467-b104-ec6dff0e806e\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]Error running evaluator <DynamicRunEvaluator evaluate_success_criteria> on run e467b425-7a8f-4961-8d24-48d8b8394b46: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-sdEW2ZAmSKu98CPwO1IA8h8q on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_42256/593233728.py\", line 45, in evaluate_success_criteria\n",
      "    responses = structured_output_model.batch([\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3399, in batch\n",
      "    inputs = step.batch(\n",
      "             ^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 5745, in batch\n",
      "    return self.bound.batch(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 920, in batch\n",
      "    return cast(\"list[Output]\", list(executor.map(invoke, inputs, configs)))\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/snap/code/205/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py\", line 619, in result_iterator\n",
      "    yield _result_or_cancel(fs.pop())\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/snap/code/205/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py\", line 317, in _result_or_cancel\n",
      "    return fut.result(timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/snap/code/205/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py\", line 456, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/snap/code/205/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/lucasraniere/snap/code/205/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 553, in _wrapped_fn\n",
      "    return contexts.pop().run(fn, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 913, in invoke\n",
      "    return self.invoke(input_, config, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1184, in _generate\n",
      "    raise e\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1152, in _generate\n",
      "    self.root_client.chat.completions.with_raw_response.parse(\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 183, in parse\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-sdEW2ZAmSKu98CPwO1IA8h8q on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "1it [02:11, 131.92s/it]Error running evaluator <DynamicRunEvaluator evaluate_success_criteria> on run 1b87427e-6048-4b2b-a7ff-662eece7a9d4: RateLimitError(\"Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-sdEW2ZAmSKu98CPwO1IA8h8q on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langsmith/evaluation/_runner.py\", line 1620, in _run_evaluators\n",
      "    evaluator_response = evaluator.evaluate_run(  # type: ignore[call-arg]\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 351, in evaluate_run\n",
      "    result = self.func(\n",
      "             ^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langsmith/evaluation/evaluator.py\", line 777, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_42256/593233728.py\", line 45, in evaluate_success_criteria\n",
      "    responses = structured_output_model.batch([\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 3399, in batch\n",
      "    inputs = step.batch(\n",
      "             ^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 5745, in batch\n",
      "    return self.bound.batch(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 920, in batch\n",
      "    return cast(\"list[Output]\", list(executor.map(invoke, inputs, configs)))\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/snap/code/205/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py\", line 619, in result_iterator\n",
      "    yield _result_or_cancel(fs.pop())\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/snap/code/205/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py\", line 317, in _result_or_cancel\n",
      "    return fut.result(timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/snap/code/205/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/snap/code/205/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/concurrent/futures/_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"/home/lucasraniere/snap/code/205/.local/share/uv/python/cpython-3.11.13-linux-x86_64-gnu/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\n",
      "    result = self.fn(*self.args, **self.kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/runnables/config.py\", line 553, in _wrapped_fn\n",
      "    return contexts.pop().run(fn, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/runnables/base.py\", line 913, in invoke\n",
      "    return self.invoke(input_, config, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 395, in invoke\n",
      "    self.generate_prompt(\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1023, in generate_prompt\n",
      "    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 840, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py\", line 1089, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1184, in _generate\n",
      "    raise e\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py\", line 1152, in _generate\n",
      "    self.root_client.chat.completions.with_raw_response.parse(\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/openai/_legacy_response.py\", line 364, in wrapped\n",
      "    return cast(LegacyAPIResponse[R], func(*args, **kwargs))\n",
      "                                      ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/openai/resources/chat/completions/completions.py\", line 183, in parse\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1259, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/lucasraniere/git/langgraph-project-deep-research/.venv/lib/python3.11/site-packages/openai/_base_client.py\", line 1047, in request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'Rate limit reached for gpt-4.1 in organization org-sdEW2ZAmSKu98CPwO1IA8h8q on requests per min (RPM): Limit 3, Used 3, Requested 1. Please try again in 20s. Visit https://platform.openai.com/account/rate-limits to learn more. You can increase your rate limit by adding a payment method to your account at https://platform.openai.com/account/billing.', 'type': 'requests', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "2it [04:04, 122.05s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<ExperimentResults Deep Research Scoping-edadbddf>"
      ],
      "text/plain": [
       "<ExperimentResults Deep Research Scoping-edadbddf>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "def target_func(inputs: dict):\n",
    "    config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "    return scope.invoke(inputs, config=config)\n",
    "\n",
    "langsmith_client.evaluate(\n",
    "    target_func,\n",
    "    data=dataset_name,\n",
    "    evaluators=[evaluate_success_criteria, evaluate_no_assumptions],\n",
    "    experiment_prefix=\"Deep Research Scoping\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6317e",
   "metadata": {},
   "source": [
    "You can click the above link to take a look at the results!\n",
    "\n",
    "Why perform evals like this?\n",
    "\n",
    "- Ensure that individual steps in your application are doing what you expect.\n",
    "- With tracing to LangSmith, you also get a sense of how long each step takes as well as the cost.\n",
    "- You can always try out cheaper, faster models to see if they can get the job done.\n",
    "\n",
    "If your agent makes mistakes, you can tune the prompts that the agent is provided to try and improve performance as well!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
